---
description: Example workflow for how to utilize `nbdev` for experimenting and exporting
  your module.
output-file: workflow.html
title: Workflow

---



<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

This workflow builds upon the process of developing a model for differentiating between pictures of cats and dogs. The source code and documentation is from https://docs.fast.ai/tutorial.vision.html.

We can use `nbdev` to seamlessly generate Python module and documentation from your experimental notebook. From this point onwards, all section represents one step, i.e., a newly modified version or an addition to your notebook from the previous step.

## Step 1 - Experimental Notebook

At this stage, the notebook is mostly for experimenting with ideas and code. The notebook itself can just be as following:  

- Install `fastai` library
- Download PETS dataset which include images of cats and dogs
- Construct dataloader from dataset
- Fine-tune pretrained model

### Notebook Content

::: {#58050604-be51-4b03-9664-dc666324cdc1 .cell}
``` {.python .cell-code}
!pip install fastai
```

::: {.cell-output .cell-output-stdout}
```
Requirement already satisfied: fastai in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (2.7.17)
Requirement already satisfied: pip in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from fastai) (24.2)
Requirement already satisfied: packaging in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from fastai) (24.1)
Requirement already satisfied: fastdownload<2,>=0.0.5 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from fastai) (0.0.7)
Requirement already satisfied: fastcore<1.8,>=1.5.29 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from fastai) (1.7.16)
Requirement already satisfied: torchvision>=0.11 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from fastai) (0.19.1)
Requirement already satisfied: matplotlib in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from fastai) (3.9.2)
Requirement already satisfied: pandas in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from fastai) (2.2.3)
Requirement already satisfied: requests in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from fastai) (2.32.3)
Requirement already satisfied: pyyaml in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from fastai) (6.0.2)
Requirement already satisfied: fastprogress>=0.2.4 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from fastai) (1.0.3)
Requirement already satisfied: pillow>=9.0.0 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from fastai) (11.0.0)
Requirement already satisfied: scikit-learn in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from fastai) (1.5.2)
Requirement already satisfied: scipy in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from fastai) (1.14.1)
Requirement already satisfied: spacy<4 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from fastai) (3.8.2)
Requirement already satisfied: torch<2.5,>=1.10 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from fastai) (2.4.1)
Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from spacy<4->fastai) (3.0.12)
Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from spacy<4->fastai) (1.0.5)
Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from spacy<4->fastai) (1.0.10)
Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from spacy<4->fastai) (2.0.8)
Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from spacy<4->fastai) (3.0.9)
Requirement already satisfied: thinc<8.4.0,>=8.3.0 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from spacy<4->fastai) (8.3.2)
Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from spacy<4->fastai) (1.1.3)
Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from spacy<4->fastai) (2.4.8)
Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from spacy<4->fastai) (2.0.10)
Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from spacy<4->fastai) (0.4.1)
Requirement already satisfied: typer<1.0.0,>=0.3.0 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from spacy<4->fastai) (0.12.5)
Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from spacy<4->fastai) (4.66.5)
Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from spacy<4->fastai) (2.9.2)
Requirement already satisfied: jinja2 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from spacy<4->fastai) (3.1.4)
Requirement already satisfied: setuptools in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from spacy<4->fastai) (75.1.0)
Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from spacy<4->fastai) (3.4.1)
Requirement already satisfied: numpy>=1.19.0 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from spacy<4->fastai) (2.0.2)
Requirement already satisfied: charset-normalizer<4,>=2 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from requests->fastai) (3.4.0)
Requirement already satisfied: idna<4,>=2.5 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from requests->fastai) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from requests->fastai) (2.2.3)
Requirement already satisfied: certifi>=2017.4.17 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from requests->fastai) (2024.8.30)
Requirement already satisfied: filelock in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from torch<2.5,>=1.10->fastai) (3.16.1)
Requirement already satisfied: typing-extensions>=4.8.0 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from torch<2.5,>=1.10->fastai) (4.12.2)
Requirement already satisfied: sympy in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from torch<2.5,>=1.10->fastai) (1.13.3)
Requirement already satisfied: networkx in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from torch<2.5,>=1.10->fastai) (3.4.1)
Requirement already satisfied: fsspec in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from torch<2.5,>=1.10->fastai) (2024.9.0)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from torch<2.5,>=1.10->fastai) (12.1.105)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from torch<2.5,>=1.10->fastai) (12.1.105)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from torch<2.5,>=1.10->fastai) (12.1.105)
Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from torch<2.5,>=1.10->fastai) (9.1.0.70)
Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from torch<2.5,>=1.10->fastai) (12.1.3.1)
Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from torch<2.5,>=1.10->fastai) (11.0.2.54)
Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from torch<2.5,>=1.10->fastai) (10.3.2.106)
Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from torch<2.5,>=1.10->fastai) (11.4.5.107)
Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from torch<2.5,>=1.10->fastai) (12.1.0.106)
Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from torch<2.5,>=1.10->fastai) (2.20.5)
Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from torch<2.5,>=1.10->fastai) (12.1.105)
Requirement already satisfied: triton==3.0.0 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from torch<2.5,>=1.10->fastai) (3.0.0)
Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<2.5,>=1.10->fastai) (12.6.77)
Requirement already satisfied: contourpy>=1.0.1 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from matplotlib->fastai) (1.3.0)
Requirement already satisfied: cycler>=0.10 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from matplotlib->fastai) (0.12.1)
Requirement already satisfied: fonttools>=4.22.0 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from matplotlib->fastai) (4.54.1)
Requirement already satisfied: kiwisolver>=1.3.1 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from matplotlib->fastai) (1.4.7)
Requirement already satisfied: pyparsing>=2.3.1 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from matplotlib->fastai) (3.2.0)
Requirement already satisfied: python-dateutil>=2.7 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from matplotlib->fastai) (2.9.0)
Requirement already satisfied: pytz>=2020.1 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from pandas->fastai) (2024.2)
Requirement already satisfied: tzdata>=2022.7 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from pandas->fastai) (2024.2)
Requirement already satisfied: joblib>=1.2.0 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from scikit-learn->fastai) (1.4.2)
Requirement already satisfied: threadpoolctl>=3.1.0 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from scikit-learn->fastai) (3.5.0)
Requirement already satisfied: language-data>=1.2 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<4->fastai) (1.2.0)
Requirement already satisfied: annotated-types>=0.6.0 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4->fastai) (0.7.0)
Requirement already satisfied: pydantic-core==2.23.4 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4->fastai) (2.23.4)
Requirement already satisfied: six>=1.5 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->fastai) (1.16.0)
Requirement already satisfied: blis<1.1.0,>=1.0.0 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.0->spacy<4->fastai) (1.0.1)
Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.0->spacy<4->fastai) (0.1.5)
Requirement already satisfied: click>=8.0.0 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<4->fastai) (8.1.7)
Requirement already satisfied: shellingham>=1.3.0 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<4->fastai) (1.5.4)
Requirement already satisfied: rich>=10.11.0 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<4->fastai) (13.9.2)
Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy<4->fastai) (0.19.0)
Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy<4->fastai) (7.0.5)
Requirement already satisfied: MarkupSafe>=2.0 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from jinja2->spacy<4->fastai) (3.0.1)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from sympy->torch<2.5,>=1.10->fastai) (1.3.0)
Requirement already satisfied: marisa-trie>=0.7.7 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4->fastai) (1.2.1)
Requirement already satisfied: markdown-it-py>=2.2.0 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4->fastai) (3.0.0)
Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4->fastai) (2.18.0)
Requirement already satisfied: wrapt in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<4->fastai) (1.16.0)
Requirement already satisfied: mdurl~=0.1 in /home/nghivo/miniconda3/envs/nbdev-test/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4->fastai) (0.1.2)
```
:::
:::


::: {#feaf83ea-1494-47c8-8d8d-7cce73d807a4 .cell}
``` {.python .cell-code}
# Import from fastai
from fastai.vision.all import *
```
:::


::: {#2b3e6d51-aef4-4377-bafe-371c24f53686 .cell}
``` {.python .cell-code}
# Download PETS dataset
path = untar_data(URLs.PETS)
path.ls()
```

::: {.cell-output .cell-output-display}

```{=html}

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
```

:::

::: {.cell-output .cell-output-display}

```{=html}

    <div>
      <progress value='811712512' class='' max='811706944' style='width:300px; height:20px; vertical-align: middle;'></progress>
      100.00% [811712512/811706944 01:10&lt;00:00]
    </div>
    
```

:::

::: {.cell-output .cell-output-display}
```
(#2) [Path('/home/nghivo/.fastai/data/oxford-iiit-pet/images'),Path('/home/nghivo/.fastai/data/oxford-iiit-pet/annotations')]
```
:::
:::


::: {#93735c93-670a-4b2c-a80d-cdb6c5d54843 .cell}
``` {.python .cell-code}
# Extract images
files = get_image_files(path/"images")
len(files)
```

::: {.cell-output .cell-output-display}
```
7390
```
:::
:::


::: {#d58e5a68-e2b5-47df-b9c7-086394526ef9 .cell}
``` {.python .cell-code}
# Create dataset loader
def label_func(f): return f[0].isupper()
dls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(224))
dls.show_batch()
```

::: {.cell-output .cell-output-display}
![](02_workflow_files/figure-html/cell-6-output-1.png){}
:::
:::


::: {#5c3e1c48-76e6-433f-925a-2b2b25938c28 .cell}
``` {.python .cell-code}
# Fine-tune model from pretrained
learn = vision_learner(dls, resnet34, metrics=error_rate)
learn.fine_tune(1)
```

::: {.cell-output .cell-output-stderr}
```
Downloading: "https://download.pytorch.org/models/resnet34-b627a593.pth" to /home/nghivo/.cache/torch/hub/checkpoints/resnet34-b627a593.pth
100%|██████████████████████████████████████████████████████████████████████████████| 83.3M/83.3M [00:07<00:00, 11.6MB/s]
```
:::

::: {.cell-output .cell-output-display}

```{=html}

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
```

:::

::: {.cell-output .cell-output-display}

```{=html}
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>error_rate</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.150972</td>
      <td>0.024604</td>
      <td>0.008796</td>
      <td>00:22</td>
    </tr>
  </tbody>
</table>
```

:::

::: {.cell-output .cell-output-display}

```{=html}

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
```

:::

::: {.cell-output .cell-output-display}

```{=html}
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>error_rate</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.057303</td>
      <td>0.012587</td>
      <td>0.005413</td>
      <td>00:28</td>
    </tr>
  </tbody>
</table>
```

:::
:::


::: {#8520fd4e-091b-4e80-a726-396212656386 .cell}
``` {.python .cell-code}
learn.show_results()
```

::: {.cell-output .cell-output-display}

```{=html}

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
```

:::

::: {.cell-output .cell-output-display}

```{=html}

```

:::

::: {.cell-output .cell-output-display}
![](02_workflow_files/figure-html/cell-8-output-3.png){}
:::
:::


::: {#ec8bcefd-73ba-40f9-b0a8-dce3f281bb1c .cell}
``` {.python .cell-code}
# Predict a single image
learn.predict(files[0])
```

::: {.cell-output .cell-output-display}

```{=html}

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
```

:::

::: {.cell-output .cell-output-display}

```{=html}

```

:::

::: {.cell-output .cell-output-display}
```
('False', tensor(0), tensor([1.0000e+00, 9.0949e-08]))
```
:::
:::


## Step 2 - Refine & Modularize Code

At this stage, suppose the developer wants to create a function that generates a predictor for single image classification between cats and dogs. The notebook now mostly involves modularizing the existing code into reusable functions, including the following:

- Class for Cat/Dog image classifier
- Function for generating from pretrained model
- Function for predicting a single image  
- Use case with test  

**NOTE**  
In this workflow, OOP (Object-Oriented Programming) is used for modularizing the code. However,it is entirely possible that we use functional programming instead. It is also possible that we skip this step and export this script as it is.

### Notebook Content

#### Experimental notebook...

#### Module

::: {#b722212c-dc7c-42fa-8b71-2921567eb183 .cell}
``` {.python .cell-code}
# Import from fastai
from fastai.vision.all import *
```
:::


::: {#6e131007-6f06-4062-848f-1e3dbaee9892 .cell}
``` {.python .cell-code}
# Original code from https://docs.fast.ai/examples/app_examples.html
# Class for Cat/Dog image
class CatOrDogImageClassifier:
    "Classifier between cat and dog images"
    
    def __init__(
        self,
        arch=resnet34,  # Pretrained model architecture
        metrics=error_rate,  # Metrics to be used
    ):
        "Generate classifier from pretrained model"
    
        # Form dataset loader
        set_seed(99, True)
        path = untar_data(URLs.PETS)/'images'
        dls = ImageDataLoaders.from_name_func(
            path, get_image_files(path), valid_pct=0.2,
            label_func=lambda x: x[0].isupper(), item_tfms=Resize(224))

        # Create and fine-tune learner
        learner = vision_learner(dls, arch, metrics=metrics).to_fp16()
        learner.fine_tune(1)
        self.learner = learner
        
    def predict(self, file_path: str):
        "Predict a single image with the learner"

        # Extract prediction with learner
        img = PILImage.create(file_path)
        prediction = self.learner.predict(img)
        label = 'Cat' if prediction[0]=='True' else 'Dog'
        dog_prob = prediction[2][0].item()
        cat_prob = prediction[2][1].item()
        
        # Print the results
        print(f"The animal in this picture is: {label}")
        print(f"Probability of 'Dog': {dog_prob:.6f}")
        print(f"Probability of 'Cat': {cat_prob:.6f}")
        
        # Return the results
        return label, dog_prob, cat_prob
```
:::


::: {#f3c50348-ca61-4afc-8217-85ffc945b05b .cell}
``` {.python .cell-code}
# Use case
classifer = CatOrDogImageClassifier()
label, dog_prob, cat_prob = classifer.predict("images/cat.jpg")

# Image should be predicted as 'Cat'
assert label=='Cat'
assert cat_prob > 0.5
```

::: {.cell-output .cell-output-display}

```{=html}

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
```

:::

::: {.cell-output .cell-output-display}

```{=html}
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>error_rate</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.147156</td>
      <td>0.022813</td>
      <td>0.008119</td>
      <td>00:16</td>
    </tr>
  </tbody>
</table>
```

:::

::: {.cell-output .cell-output-display}

```{=html}

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
```

:::

::: {.cell-output .cell-output-display}

```{=html}
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>error_rate</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.051962</td>
      <td>0.031823</td>
      <td>0.011502</td>
      <td>00:20</td>
    </tr>
  </tbody>
</table>
```

:::

::: {.cell-output .cell-output-display}

```{=html}

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
```

:::

::: {.cell-output .cell-output-display}

```{=html}

```

:::

::: {.cell-output .cell-output-stdout}
```
The animal in this picture is: Cat
Probability of 'Dog': 0.000073
Probability of 'Cat': 0.999927
```
:::
:::


## Step 3 - Export Module

At this stage, the developer needs to export these functions for future usage. `nbdev` provides an easy option for turning the written code in Jupyter Notebook into installable Python module. The export will involve multiple files, including the previous notebook. The changes to be made are as follows:

- Add all external libraries to `setting.ini`  
- Add `#| default_exp <name>` to notebook to export into a new file with similar name
- Add `#| export` to all relevant cells in notebook  
- Use `nbdev_export` in CLI or add Jupyter Notebook cell for exporting with `nbdev.nbdev_export()`  
- Check the exported module
- Install exported module in developer mode with `pip install -e '.[dev]'` (if not already)
- Use the exported module with `from nbdev_test.<name> import *`

**NOTE**  
If encountering errors in importing the new module, try:  
- Running `pip install -e .` again  
- Restarting Jupyter notebook or Python kernel  
- Running `nbdev_prepare` in CLI

### Change in `setting.ini`

All external libraries are to be added into the `setting.ini` file. In this workflow, we are only using `fastai` library version `2.7.17`. To set this requirement in `setting.ini`, we need to add a line for requirement (or uncommenting the default requirement line):

```
### Optional ###
requirements = fastai
```

If we want to specify version (recommended to avoid future compatibility issues), we can set this to:
```
requirements = fastai==2.7.17
```

or
```
requirements = fastai>=2.7.17
```

If there are new libraries, add them to the same line, separated by '<SPACE>'. For example:
```
requirements = fastai pandas
```

### Notebook Content

#### Experimental notebook...

#### Module

---

[source](https://github.com/ninjalabo/nbdev-test/blob/main/nbdev_test/classifer.py#L59){target="_blank" style="float:right; font-size:smaller"}

### CatOrDogImageClassifier

>      CatOrDogImageClassifier (arch=<function resnet34>, metrics=<function
>                               error_rate>)

*Classifier between cat and dog images*

|    | **Type** | **Default** | **Details** |
| -- | -------- | ----------- | ----------- |
| arch | function | resnet34 | Pretrained model architecture |
| metrics | function | error_rate | Metrics to be used |


::: {#09d9588b-b1ed-45ab-8520-63742b95a919 .cell}
``` {.python .cell-code}
# NOTE: it is not required to export this use case with test.
# Use case
classifer = CatOrDogImageClassifier()
label, dog_prob, cat_prob = classifer.predict("images/cat.jpg")

# Image should be predicted as 'Cat'
assert label=='Cat'
assert cat_prob > 0.5
```

::: {.cell-output .cell-output-display}

```{=html}

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
```

:::

::: {.cell-output .cell-output-display}

```{=html}
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>error_rate</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.147156</td>
      <td>0.022813</td>
      <td>0.008119</td>
      <td>00:13</td>
    </tr>
  </tbody>
</table>
```

:::

::: {.cell-output .cell-output-display}

```{=html}

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
```

:::

::: {.cell-output .cell-output-display}

```{=html}
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>error_rate</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.051962</td>
      <td>0.031823</td>
      <td>0.011502</td>
      <td>00:17</td>
    </tr>
  </tbody>
</table>
```

:::

::: {.cell-output .cell-output-display}

```{=html}

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
```

:::

::: {.cell-output .cell-output-display}

```{=html}

```

:::

::: {.cell-output .cell-output-stdout}
```
The animal in this picture is: Cat
Probability of 'Dog': 0.000073
Probability of 'Cat': 0.999927
```
:::
:::


::: {#ad44c38b-4e39-44c3-9c36-a3e72bf7042d .cell}
``` {.python .cell-code}
# Export via cell
from nbdev import nbdev_export
nbdev_export()
```
:::


### Change in `nbdev_test` module

After running the previous notebook, there should be a new file named `classifier.py` in the `nbdev_test` directory. This new file should include all code from cells with the `#| export` marker.

## Step 4 - Create Documentation

At this stage, the developer wants to generate a full documentation of all this process. `nbdev` simplifies this process with `nbdev_preview` which generates Quarto Pages for documentation from the notebook. We need to prepare the notebook for documentation generation which should include:

- Add narration with markdown cell  
- Add directives to cells to determine which cell is visible:  
  - Add `#| export` for adding the cell to the module & documentation  
  - Add `#| hide` for testing cells and `nbdev` operations cells  
  - For more options, check out the official documents: https://nbdev.fast.ai/explanations/directives.html  
- Use `show_doc` for displaying class / functions
- Use `nbdev_preview` for previewing documentation
- Use `nbdev_docs` for generating documentation in `.html` format

**NOTE**  
After this running the notebook content in this section, the previously exported code will be duplicate as they are being exported to the same file. Usually, one notebook will generate one exported module.

### Notebook Content 

::: {#a482ca56-1831-40be-bfff-dc2f2388f55e .cell}
``` {.python .cell-code code-fold="show" code-summary="Exported source"}
from fastai.vision.all import *
```
:::


For this task, we will use the Oxford-IIIT Pet Dataset that contains images of cats and dogs of 37 different breeds. We will first show how to build a simple cat-vs-dog classifier, then a little bit more advanced model that can classify all breeds.

The dataset can be downloaded and decompressed with this line of code:

::: {#6c506b04-20e6-45d3-9796-3662a65d5a69 .cell}
``` {.python .cell-code}
path = untar_data(URLs.PETS)
```
:::


It will only do this download once, and return the location of the decompressed archive. We can check what is inside with the `.ls()` method.

We will ignore the annotations folder for now, and focus on the images one. get_image_files is a fastai function that helps us grab all the image files (recursively) in one folder.

::: {#687fc7ae-d403-433e-b1b5-8e76e4413642 .cell}
``` {.python .cell-code}
files = get_image_files(path/"images")
len(files)
```

::: {.cell-output .cell-output-display}
```
7390
```
:::
:::


To label our data for the cats vs dogs problem, we need to know which filenames are of dog pictures and which ones are of cat pictures. There is an easy way to distinguish: the name of the file begins with a capital for cats, and a lowercased letter for dogs.

We can then define an easy label function:

::: {#30e03379-8d69-4879-ad7c-681b99d52411 .cell}
``` {.python .cell-code}
def label_func(f): return f[0].isupper()
```
:::


To get our data ready for a model, we need to put it in a DataLoaders object. Here we have a function that labels using the file names, so we will use ImageDataLoaders.from_name_func. There are other factory methods of ImageDataLoaders that could be more suitable for your problem, so make sure to check them all in vision.data.dls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(224))

::: {#b4447ab7-0a51-4657-891b-f67e03e67a72 .cell}
``` {.python .cell-code}
dls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(224))
```
:::


We have passed to this function the directory we’re working in, the files we grabbed, our label_func and one last piece as item_tfms: this is a Transform applied on all items of our dataset that will resize each image to 224 by 224, by using a random crop on the largest dimension to make it a square, then resizing to 224 by 224. If we didn’t pass this, we would get an error later as it would be impossible to batch the items together.

We can then check if everything looks okay with the show_batch method (True is for cat, False is for dog):

::: {#2a8c3313-ffa2-4dfc-ae1b-0ceff8c07476 .cell}
``` {.python .cell-code}
dls.show_batch()
```

::: {.cell-output .cell-output-display}
![](02_workflow_files/figure-html/cell-21-output-1.png){}
:::
:::


Then we can create a Learner, which is a fastai object that combines the data and a model for training, and uses transfer learning to fine tune a pretrained model in just two lines of code:

::: {#13153f54-419a-49b4-ab8c-dbd1045fa976 .cell}
``` {.python .cell-code}
learn = vision_learner(dls, resnet34, metrics=error_rate)
learn.fine_tune(1)
```

::: {.cell-output .cell-output-display}

```{=html}

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
```

:::

::: {.cell-output .cell-output-display}

```{=html}
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>error_rate</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.150823</td>
      <td>0.023608</td>
      <td>0.009472</td>
      <td>00:23</td>
    </tr>
  </tbody>
</table>
```

:::

::: {.cell-output .cell-output-display}

```{=html}

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
```

:::

::: {.cell-output .cell-output-display}

```{=html}
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>error_rate</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.053443</td>
      <td>0.007796</td>
      <td>0.002706</td>
      <td>00:29</td>
    </tr>
  </tbody>
</table>
```

:::
:::


The first line downloaded a model called ResNet34, pretrained on ImageNet, and adapted it to our specific problem. It then fine tuned that model and in a relatively short time, we get a model with an error rate of well under 1%… amazing!

If you want to make a prediction on a new image, you can use learn.predict:

::: {#e7923cf9-bc44-4139-b98a-a09dc920b74a .cell}
``` {.python .cell-code}
learn.predict(files[0])
```

::: {.cell-output .cell-output-display}

```{=html}

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
```

:::

::: {.cell-output .cell-output-display}

```{=html}

```

:::

::: {.cell-output .cell-output-display}
```
('False', tensor(0), tensor([9.9995e-01, 5.1299e-05]))
```
:::
:::


The predict method returns three things: the decoded prediction (here False for dog), the index of the predicted class and the tensor of probabilities of all classes in the order of their indexed labels(in this case, the model is quite confident about the being that of a dog). This method accepts a filename, a PIL image or a tensor directly in this case. We can also have a look at some predictions with the show_results method:

::: {#a8299461-6428-4722-ac2e-32c251e3612b .cell}
``` {.python .cell-code}
learn.show_results()
```

::: {.cell-output .cell-output-display}

```{=html}

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
```

:::

::: {.cell-output .cell-output-display}

```{=html}

```

:::

::: {.cell-output .cell-output-display}
![](02_workflow_files/figure-html/cell-24-output-3.png){}
:::
:::


From this, we can build a classifer for images of cats and dogs:

---

[source](https://github.com/ninjalabo/nbdev-test/blob/main/nbdev_test/classifer.py#L59){target="_blank" style="float:right; font-size:smaller"}

### CatOrDogImageClassifier

>      CatOrDogImageClassifier (arch=<function resnet34>, metrics=<function
>                               error_rate>)

*Classifier between cat and dog images*

|    | **Type** | **Default** | **Details** |
| -- | -------- | ----------- | ----------- |
| arch | function | resnet34 | Pretrained model architecture |
| metrics | function | error_rate | Metrics to be used |


---

[source](https://github.com/ninjalabo/nbdev-test/blob/main/nbdev_test/classifer.py#L81){target="_blank" style="float:right; font-size:smaller"}

### CatOrDogImageClassifier.predict

>      CatOrDogImageClassifier.predict (file_path:str)

*Predict a single image with the learner*


## Tips

- **Module Packaging:** Each notebook will only export 1 module (or `.py` file), which is defined by directive `#| default_exp <file_name>`. However, it is possible to create sub-module with this directive. For instance, if the `file_name` is defined as `classifier.dog_and_cat`, then the created module will be exported to file `classifier/dog_and_cat.py` and a new directory `classifier` is automatically created in your main module directory.  
- **Notebook Packaging:** Each notebook will be automatically rendered into a Quarto Page. The order of the notebook is automatically inferred from the title (complying with naming convention: `01_notebook1.ipynb`, `02_notebook2.ipynb`. It is also possible to create sections in Quarto Pages by creating a new child directory in the `nbs` folder with the section name and adding corresponding notebooks to it. It is also possible to customize content structure by editing the `sidebar.yml` file.  
- **Jupyter Lab Support:** As we are creating module, it can be beneficial to have type hinting and code styling. Check https://chatgpt.com/share/67120f9f-6c58-8002-b10f-7c18d051d2f0 for how to enable such utilities, as well as customizing them for your own environment.


